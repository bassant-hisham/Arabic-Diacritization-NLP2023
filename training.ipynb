{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Team Members</h2>\n",
    "<ul>\n",
    "<li>Sohad Hossam Eldin 1190019</li>\n",
    "<li>Bassant Hisham Mohamed 1190018</li>\n",
    "<li>Yasmin Hashem Niazy 4200013</li>\n",
    "<li>Mary Ashraf 1190322</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sohad\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sohad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        dataset_txt = open(path, \"r\", encoding='utf-8').read()\n",
    "        #print(type(dataset_txt))\n",
    "\n",
    "        chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-]+\\s*/\\s*[a-zA-Z0-9_-]+\\s*\\)|[a-zA-Z0-9_-]+\" \n",
    "        dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "        dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*\",dataset_cleaned)\n",
    "\n",
    "        self.x_train_letters,self.y_train_letters,self.x_train_words,self.y_train_words, y_vocab = self.Tokenizing(dataset_sentences)\n",
    "\n",
    "        word_index = {word: idx + 1 for idx, word in enumerate(y_vocab)}\n",
    "        y_train_indices = [[word_index[word] for word in sentence] for sentence in self.y_train_words]\n",
    "        self.y_train_padded = tf.keras.preprocessing.sequence.pad_sequences(y_train_indices, padding='post')\n",
    "        \n",
    "        # self.y_train_letters_numbered = LabelEncoder().fit_transform(y_train_letters)\n",
    "        # self.y_train_words_numbered = LabelEncoder().fit_transform(y_train_words)\n",
    "\n",
    "        # #dictionary of the labels(words,letters) before converted to numbers\n",
    "        # self.encoding_mapping_y_letters = {encoded_label: original_label for encoded_label, original_label  in zip(self.y_train_letters_numbered, y_train_letters)}\n",
    "        # self.encoding_mapping_y_words = {encoded_label: original_label for encoded_label, original_label  in zip( self.y_train_words_numbered, y_train_words)}  \n",
    "\n",
    "        # print(self.y_train_letters_numbered.shape)\n",
    "        # print(self.y_train_words_numbered.shape)\n",
    "        # print( self.y_train_letters_numbered[:10])\n",
    "        # print( self.y_train_words_numbered[:10])\n",
    "\n",
    "        # self.y_train_letters_numbered = self.y_train_letters_numbered.reshape((len(self.y_train_letters_numbered), 1))\n",
    "        # self.y_train_words_numbered = self.y_train_words_numbered.reshape((len(self.y_train_words_numbered), 1))\n",
    "\n",
    "        # print(self.y_train_letters_numbered.shape)\n",
    "        # print(self.y_train_words_numbered.shape)\n",
    "        # print( self.y_train_letters_numbered[:10])\n",
    "        # print( self.y_train_words_numbered[:10])\n",
    "\n",
    "\n",
    "    def Tokenizing(self,dataset_sentences):\n",
    "\n",
    "        x_train_letters = []\n",
    "        y_train_letters = []\n",
    "        x_train_words = []\n",
    "        y_train_words = []\n",
    "        y_vocab = set()\n",
    "        for sentence in dataset_sentences:\n",
    "            sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "            tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "            tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "\n",
    "            for word in tokens:\n",
    "                text, inputs, diacritics =util.extract_haraqat(word)\n",
    "                # inputs.insert(0, \"<s>\")\n",
    "                # inputs.append(\"</s>\")\n",
    "\n",
    "                # diacritics.insert(0, \"<s>\")\n",
    "                # diacritics.append(\"</s>\")\n",
    "                \n",
    "                x_train_letters.append(inputs)\n",
    "                y_train_letters.append(diacritics)\n",
    "\n",
    "\n",
    "            if(len(tokens)) :\n",
    "                tokens.insert(0, \"<s>\")\n",
    "                tokens.append(\"</s>\")\n",
    "                y_train_words.append(tokens)\n",
    "                y_vocab.update(tokens)\n",
    "            if(len(tokens_wihtout_diacratics)) :\n",
    "                tokens_wihtout_diacratics.insert(0, \"<s>\")\n",
    "                tokens_wihtout_diacratics.append(\"</s>\")\n",
    "                x_train_words.append(tokens_wihtout_diacratics)\n",
    "\n",
    "        print(y_train_words[0:10])\n",
    "        return x_train_letters,y_train_letters,x_train_words,y_train_words, y_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train_letters), len(self.x_train_words)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train_letters[idx], self.y_train_letters_numbered[idx], self.x_train_words[idx], self.y_train_words_numbered[idx]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    # def get_splits(self, n_test=0.33):\n",
    "    #     # determine sizes\n",
    "    #     test_size = round(n_test * len(self.X))\n",
    "    #     train_size = len(self.X) - test_size\n",
    "    #     # calculate the split\n",
    "    #     return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reading the dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_txt = open(r\"train.txt\", \"r\", encoding='utf-8').read()\n",
    "# print(type(dataset_txt))\n",
    "#print(dataset_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cleaning the dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need to remove [ :ص]\n",
    "# chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-]+\\s*/\\s*[a-zA-Z0-9_-]+\\s*\\)|[a-zA-Z0-9_-]+\" \n",
    "# dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "#print(dataset_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Segmenting dataset into sentences</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r'(\\([^)]*\\))' should we remove the brackets or not\n",
    "# dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*\",dataset_cleaned)\n",
    "# print(type(dataset_sentences))\n",
    "# print(dataset_sentences[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tokenizing the sentence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_letters = []\n",
    "# y_train_letters = []\n",
    "# x_train_words = []\n",
    "# y_train_words = []\n",
    "# for sentence in dataset_sentences:\n",
    "#     sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "#     tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "#     tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "   \n",
    "#     for word in tokens:\n",
    "#         text, inputs, diacritics =util.extract_haraqat(word)\n",
    "#         x_train_letters.append(inputs)\n",
    "#         y_train_letters.append(diacritics)\n",
    "\n",
    "#     if(len(tokens)) :\n",
    "#         tokens.insert(0, \"<s>\")\n",
    "#         tokens.append(\"</s>\")\n",
    "#         y_train_words.append(tokens)\n",
    "#     if(len(tokens_wihtout_diacratics)) :\n",
    "#         tokens_wihtout_diacratics.insert(0, \"<s>\")\n",
    "#         tokens_wihtout_diacratics.append(\"</s>\")\n",
    "#         x_train_words.append(tokens_wihtout_diacratics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_letters[0:10])\n",
    "# print(y_train_letters[0:10])\n",
    "# print(y_train_words[0:10])\n",
    "# print(x_train_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train_letters))\n",
    "# print(x_train_letters[2])\n",
    "\n",
    "#ouh lala , tokinze is dividing the tashkelat as indpendent letters as well , what does that tell us tho "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = ArabicDataset(r\"train.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'قَوْلُهُ', '</s>'], ['<s>', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', '</s>'], ['<s>', 'قَالَ', 'الزَّرْكَشِيُّ', '</s>'], ['<s>', 'ابْنُ', 'عَرَفَةَ', '</s>'], ['<s>', 'قَوْلُهُ', '</s>'], ['<s>', 'بِلَفْظٍ', 'يَقْتَضِيه', 'كَإِنْكَارِ', 'غَيْرِ', 'حَدِيثٍ', 'بِالْإِسْلَامِ', 'وُجُوبَ', 'مَا', 'عُلِمَ', 'وُجُوبُهُ', 'مِنْ', 'الدِّينِ', 'ضَرُورَةً', '</s>'], ['<s>', 'كَإِلْقَاءِ', 'مُصْحَفٍ', 'بِقَذَرٍ', 'وَشَدِّ', 'زُنَّارٍ', '</s>'], ['<s>', 'ابْنُ', 'عَرَفَةَ', '</s>'], ['<s>', 'قَوْلُ', 'ابْنِ', 'شَاسٍ', '</s>'], ['<s>', 'أَوْ', 'بِفِعْلٍ', 'يَتَضَمَّنُهُ', 'هُوَ', 'كَلُبْسِ', 'الزُّنَّارِ', 'وَإِلْقَاءِ', 'الْمُصْحَفِ', 'فِي', 'صَرِيحِ', 'النَّجَاسَةِ', 'وَالسُّجُودِ', 'لِلصَّنَمِ', 'وَنَحْوِ', 'ذَلِكَ', '</s>']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(272126, 255)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ArabicDataset(r\"train.txt\")\n",
    "(272126, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.y_train_letters_numbered[:10])\n",
    "# print(dataset.y_train_words_numbered[:10])\n",
    "# print(dataset.x_train_words[:10])\n",
    "# print(dataset.x_train_letters[:10])\n",
    "\n",
    "model = Word2Vec(sentences=dataset.x_train_words, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save(\"my_word2vec_model\")\n",
    "\n",
    "# specific_word = 'قوله'\n",
    "# print(f\"{specific_word}: {model.wv[specific_word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(\"my_word2vec_model\")\n",
    "\n",
    "# + 1 is added so that the padding has a representation in the embedding matrix\n",
    "embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size))\n",
    "\n",
    "print(word2vec_model.wv.index_to_key[0])\n",
    "\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "# embedding_matrix = word2vec_model.wv.vectors\n",
    "# print(embedding_matrix.shape)\n",
    "print(embedding_matrix.shape)\n",
    "print(len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size)\n",
    "\n",
    "x_train_indices = [[word_index[word] for word in sentence] for sentence in dataset.x_train_words]\n",
    "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train_indices, padding='post')\n",
    "\n",
    "# # print(x_train_padded[0])\n",
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], input_length=x_train_padded.shape[1], trainable=False))\n",
    "first_bidirectional_layer = Bidirectional(LSTM(units=10, return_sequences=True))\n",
    "bi_lstm_model.add(first_bidirectional_layer)\n",
    "# bi_lstm_model.add(Dense(255, activation='sigmoid'))\n",
    "\n",
    "# bi_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# bi_lstm_model.fit(x_train_padded, dataset.y_train_padded, epochs=1, batch_size=32)\n",
    "lstm_output = bi_lstm_model.predict(x_train_padded)\n",
    "print(type(lstm_output))\n",
    "\n",
    "# word2vec_model.vector_size --> this is the same as saying embedding_matrix.shape[1]\n",
    "# keep into considiration the trainable = true parameter\n",
    "# there is a possibility to pad with the end delimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm_output.shape)\n",
    "print(x_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = bi_lstm_model.evaluate(x_train_padded, dataset.y_train_padded)\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "bi_lstm_model.save(\"bi-lstm-word2vec-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input = Input(shape=(max_word_length,))\n",
    "word_embedding = Embedding(input_dim=num_words, output_dim=embedding_dim)(word_input)\n",
    "word_lstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(word_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
