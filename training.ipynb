{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Team Members</h2>\n",
    "<ul>\n",
    "<li>Sohad Hossam Eldin 1190019</li>\n",
    "<li>Bassant Hisham Mohamed 1190018</li>\n",
    "<li>Yasmin Hashem Niazy 4200013</li>\n",
    "<li>Mary Ashraf 1190322</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sohad\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sohad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        dataset_txt = open(path, \"r\", encoding='utf-8').read()\n",
    "        #print(type(dataset_txt))\n",
    "\n",
    "        chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-`]+\\s*/\\s*[a-zA-Z0-9_-`]+\\s*\\)|[a-zA-Z0-9_-]+|-|`|–|~|\\u200f|'\" \n",
    "        dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "        dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*|\\s*\\[\\s*|\\s*\\]\\s*|\\s*{\\s*|\\s*}\\s*|\\s*\\*\\s*|\\s*»\\s*|\\s*«\\s*|\\s*\\\\\\s*|\\s*/\\s*|\\s*;\\s*|\\s*,\\s*\",dataset_cleaned)\n",
    "\n",
    "        self.x_train_letters,self.y_train_letters,self.x_train_words,self.y_train_words, y_vocab = self.Tokenizing(dataset_sentences)\n",
    "\n",
    "        word_index = {word: idx + 1 for idx, word in enumerate(y_vocab)}\n",
    "        y_train_indices = [[word_index[word] for word in sentence] for sentence in self.y_train_words]\n",
    "        self.y_train_padded = tf.keras.preprocessing.sequence.pad_sequences(y_train_indices, padding='post')\n",
    "        \n",
    "\n",
    "        self.harakat  = {\n",
    "                            \"\": 0,       # No Diacritic\n",
    "                            \"َ\": 1,      # Fatha\n",
    "                            \"ً\": 2,      # Fathatah\n",
    "                            \"ُ\": 3,      # Damma\n",
    "                            \"ٌ\": 4,      # Dammatan\n",
    "                            \"ِ\": 5,      # Kasra\n",
    "                            \"ٍ\": 6,      # Kasratan\n",
    "                            \"ْ\": 7,      # Sukun\n",
    "                            \"ّ\": 8,      # Shaddah\n",
    "                            \"َّ\": 9,     # Shaddah + Fatha\n",
    "                            \"ًّ\": 10,    # Shaddah + Fathatah\n",
    "                            \"ُّ\": 11,    # Shaddah + Damma\n",
    "                            \"ٌّ\": 12,    # Shaddah + Dammatan\n",
    "                            \"ِّ\": 13,    # Shaddah + Kasra\n",
    "                            \"ٍّ\": 14,    # Shaddah + Kasratan\n",
    "                            \n",
    "                        }\n",
    "        \n",
    "        self.arabic_alphabet = {\n",
    "                                \"ا\": 1,\n",
    "                                \"ب\": 2,\n",
    "                                \"ت\": 3,\n",
    "                                \"ث\": 4,\n",
    "                                \"ج\": 5,\n",
    "                                \"ح\": 6,\n",
    "                                \"خ\": 7,\n",
    "                                \"د\": 8,\n",
    "                                \"ذ\": 9,\n",
    "                                \"ر\": 10,\n",
    "                                \"ز\": 11,\n",
    "                                \"س\": 12,\n",
    "                                \"ش\": 13,\n",
    "                                \"ص\": 14,\n",
    "                                \"ض\": 15,\n",
    "                                \"ط\": 16,\n",
    "                                \"ظ\": 17,\n",
    "                                \"ع\": 18,\n",
    "                                \"غ\": 19,\n",
    "                                \"ف\": 20,\n",
    "                                \"ق\": 21,\n",
    "                                \"ك\": 22,\n",
    "                                \"ل\": 23,\n",
    "                                \"م\": 24,\n",
    "                                \"ن\": 25,\n",
    "                                \"ه\": 26,\n",
    "                                \"و\": 27,\n",
    "                                \"ي\": 28,\n",
    "                                \"آ\": 29,\n",
    "                                \"إ\": 30,\n",
    "                                \"ئ\": 31,\n",
    "                                \"ء\": 32,\n",
    "                                \"أ\": 33,\n",
    "                                \"ؤ\":34,\n",
    "                                \"ة\":35,\n",
    "                                \"ى\":36,\n",
    "                            }\n",
    "\n",
    "        # self.y_train_letters_numbered = LabelEncoder().fit_transform(y_train_letters)\n",
    "        # self.y_train_words_numbered = LabelEncoder().fit_transform(y_train_words)\n",
    "\n",
    "        # #dictionary of the labels(words,letters) before converted to numbers\n",
    "        # self.encoding_mapping_y_letters = {encoded_label: original_label for encoded_label, original_label  in zip(self.y_train_letters_numbered, y_train_letters)}\n",
    "        # self.encoding_mapping_y_words = {encoded_label: original_label for encoded_label, original_label  in zip( self.y_train_words_numbered, y_train_words)}  \n",
    "\n",
    "        # print(self.y_train_letters_numbered.shape)\n",
    "        # print(self.y_train_words_numbered.shape)\n",
    "        # print( self.y_train_letters_numbered[:10])\n",
    "        # print( self.y_train_words_numbered[:10])\n",
    "\n",
    "        # self.y_train_letters_numbered = self.y_train_letters_numbered.reshape((len(self.y_train_letters_numbered), 1))\n",
    "        # self.y_train_words_numbered = self.y_train_words_numbered.reshape((len(self.y_train_words_numbered), 1))\n",
    "\n",
    "        # print(self.y_train_letters_numbered.shape)\n",
    "        # print(self.y_train_words_numbered.shape)\n",
    "        # print( self.y_train_letters_numbered[:10])\n",
    "        # print( self.y_train_words_numbered[:10])\n",
    "\n",
    "\n",
    "    def Tokenizing(self,dataset_sentences):\n",
    "\n",
    "        x_train_letters = []\n",
    "        y_train_letters = []\n",
    "        x_train_words = []\n",
    "        y_train_words = []\n",
    "        y_vocab = set()\n",
    "        for sentence in dataset_sentences:\n",
    "            sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "            tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "            tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "\n",
    "            for word in tokens:\n",
    "                text, inputs, diacritics =util.extract_haraqat(word)\n",
    "                # inputs.insert(0, \"<s>\")\n",
    "                # inputs.append(\"</s>\")\n",
    "\n",
    "                # diacritics.insert(0, \"<s>\")\n",
    "                # diacritics.append(\"</s>\")\n",
    "                \n",
    "                x_train_letters.append(inputs)\n",
    "                y_train_letters.extend(diacritics)\n",
    "\n",
    "\n",
    "            if(len(tokens)) :\n",
    "                tokens.insert(0, \"<s>\")\n",
    "                tokens.append(\"</s>\")\n",
    "                y_train_words.append(tokens)\n",
    "                y_vocab.update(tokens)\n",
    "            if(len(tokens_wihtout_diacratics)) :\n",
    "                tokens_wihtout_diacratics.insert(0, \"<s>\")\n",
    "                tokens_wihtout_diacratics.append(\"</s>\")\n",
    "                x_train_words.append(tokens_wihtout_diacratics)\n",
    "\n",
    "        print(y_train_words[0:10])\n",
    "        print(y_train_letters[0:10])\n",
    "        print(x_train_letters[0:10])\n",
    "        return x_train_letters,y_train_letters,x_train_words,y_train_words, y_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train_letters), len(self.x_train_words)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train_letters[idx], self.y_train_letters_numbered[idx], self.x_train_words[idx], self.y_train_words_numbered[idx]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    # def get_splits(self, n_test=0.33):\n",
    "    #     # determine sizes\n",
    "    #     test_size = round(n_test * len(self.X))\n",
    "    #     train_size = len(self.X) - test_size\n",
    "    #     # calculate the split\n",
    "    #     return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reading the dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_txt = open(r\"train.txt\", \"r\", encoding='utf-8').read()\n",
    "# print(type(dataset_txt))\n",
    "#print(dataset_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cleaning the dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need to remove [ :ص]\n",
    "# chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-]+\\s*/\\s*[a-zA-Z0-9_-]+\\s*\\)|[a-zA-Z0-9_-]+\" \n",
    "# dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "#print(dataset_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Segmenting dataset into sentences</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r'(\\([^)]*\\))' should we remove the brackets or not\n",
    "# dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*\",dataset_cleaned)\n",
    "# print(type(dataset_sentences))\n",
    "# print(dataset_sentences[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tokenizing the sentence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_letters = []\n",
    "# y_train_letters = []\n",
    "# x_train_words = []\n",
    "# y_train_words = []\n",
    "# for sentence in dataset_sentences:\n",
    "#     sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "#     tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "#     tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "   \n",
    "#     for word in tokens:\n",
    "#         text, inputs, diacritics =util.extract_haraqat(word)\n",
    "#         x_train_letters.append(inputs)\n",
    "#         y_train_letters.append(diacritics)\n",
    "\n",
    "#     if(len(tokens)) :\n",
    "#         tokens.insert(0, \"<s>\")\n",
    "#         tokens.append(\"</s>\")\n",
    "#         y_train_words.append(tokens)\n",
    "#     if(len(tokens_wihtout_diacratics)) :\n",
    "#         tokens_wihtout_diacratics.insert(0, \"<s>\")\n",
    "#         tokens_wihtout_diacratics.append(\"</s>\")\n",
    "#         x_train_words.append(tokens_wihtout_diacratics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_letters[0:10])\n",
    "# print(y_train_letters[0:10])\n",
    "# print(y_train_words[0:10])\n",
    "# print(x_train_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train_letters))\n",
    "# print(x_train_letters[2])\n",
    "\n",
    "#ouh lala , tokinze is dividing the tashkelat as indpendent letters as well , what does that tell us tho "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = ArabicDataset(r\"train.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'قَوْلُهُ', '</s>'], ['<s>', 'أَوْ', 'قَطَعَ', 'الْأَوَّلُ', 'يَدَهُ', 'إلَخْ', '</s>'], ['<s>', 'قَالَ', 'الزَّرْكَشِيُّ', '</s>'], ['<s>', 'ابْنُ', 'عَرَفَةَ', '</s>'], ['<s>', 'قَوْلُهُ', '</s>'], ['<s>', 'بِلَفْظٍ', 'يَقْتَضِيه', 'كَإِنْكَارِ', 'غَيْرِ', 'حَدِيثٍ', 'بِالْإِسْلَامِ', 'وُجُوبَ', 'مَا', 'عُلِمَ', 'وُجُوبُهُ', 'مِنْ', 'الدِّينِ', 'ضَرُورَةً', '</s>'], ['<s>', 'كَإِلْقَاءِ', 'مُصْحَفٍ', 'بِقَذَرٍ', 'وَشَدِّ', 'زُنَّارٍ', '</s>'], ['<s>', 'ابْنُ', 'عَرَفَةَ', '</s>'], ['<s>', 'قَوْلُ', 'ابْنِ', 'شَاسٍ', '</s>'], ['<s>', 'أَوْ', 'بِفِعْلٍ', 'يَتَضَمَّنُهُ', 'هُوَ', 'كَلُبْسِ', 'الزُّنَّارِ', 'وَإِلْقَاءِ', 'الْمُصْحَفِ', 'فِي', 'صَرِيحِ', 'النَّجَاسَةِ', 'وَالسُّجُودِ', 'لِلصَّنَمِ', 'وَنَحْوِ', 'ذَلِكَ', '</s>']]\n",
      "['َ', 'ْ', 'ُ', 'ُ', 'َ', 'ْ', 'َ', 'َ', 'َ', '']\n",
      "[['ق', 'و', 'ل', 'ه'], ['أ', 'و'], ['ق', 'ط', 'ع'], ['ا', 'ل', 'أ', 'و', 'ل'], ['ي', 'د', 'ه'], ['إ', 'ل', 'خ'], ['ق', 'ا', 'ل'], ['ا', 'ل', 'ز', 'ر', 'ك', 'ش', 'ي'], ['ا', 'ب', 'ن'], ['ع', 'ر', 'ف', 'ة']]\n"
     ]
    }
   ],
   "source": [
    "dataset = ArabicDataset(r\"train.txt\")\n",
    "# (272126, 255)\n",
    "# (272126, 255, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.y_train_letters_numbered[:10])\n",
    "# print(dataset.y_train_words_numbered[:10])\n",
    "# print(dataset.x_train_words[:10])\n",
    "# print(dataset.x_train_letters[:10])\n",
    "\n",
    "model = Word2Vec(sentences=dataset.x_train_words, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save(\"my_word2vec_model\")\n",
    "\n",
    "# specific_word = 'قوله'\n",
    "# print(f\"{specific_word}: {model.wv[specific_word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "(105734, 100)\n",
      "105734 100\n",
      "WARNING:tensorflow:From c:\\Users\\sohad\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "8701/8701 [==============================] - 272s 31ms/step\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = Word2Vec.load(\"my_word2vec_model\")\n",
    "\n",
    "# + 1 is added so that the padding has a representation in the embedding matrix\n",
    "embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size))\n",
    "\n",
    "print(word2vec_model.wv.index_to_key[0])\n",
    "\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "# embedding_matrix = word2vec_model.wv.vectors\n",
    "# print(embedding_matrix.shape)\n",
    "print(embedding_matrix.shape)\n",
    "print(len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size)\n",
    "\n",
    "x_train_indices = [[word_index[word] for word in sentence] for sentence in dataset.x_train_words]\n",
    "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train_indices, padding='post')\n",
    "\n",
    "# # print(x_train_padded[0])\n",
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights=[embedding_matrix], input_length=x_train_padded.shape[1], trainable=False))\n",
    "first_bidirectional_layer = Bidirectional(LSTM(units=10, return_sequences=True))\n",
    "bi_lstm_model.add(first_bidirectional_layer)\n",
    "# bi_lstm_model.add(Dense(255, activation='sigmoid'))\n",
    "\n",
    "# bi_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# bi_lstm_model.fit(x_train_padded, dataset.y_train_padded, epochs=1, batch_size=32)\n",
    "lstm_output = bi_lstm_model.predict(x_train_padded)\n",
    "print(type(lstm_output))\n",
    "\n",
    "# word2vec_model.vector_size --> this is the same as saying embedding_matrix.shape[1]\n",
    "# keep into considiration the trainable = true parameter\n",
    "# there is a possibility to pad with the end delimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lstm_output[1][0:12])\n",
    "# print(x_train_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing second model for char level encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over lstm_output to remove unwanted feature vectors\n",
    "no_of_sentences, length_of_sentence, feature_vector_size = lstm_output.shape\n",
    "\n",
    "lstm_output_without_padding = list()\n",
    "for i in range(no_of_sentences):\n",
    "    lstm_output_without_padding.extend(lstm_output[i, 1:len(dataset.x_train_words[i])-1, :])\n",
    "\n",
    "# print(lstm_output_without_padding[0:6])\n",
    "\n",
    "one_hot_enconding = np.eye(37)\n",
    "# x_train_letter_concatinated = [[np.concatenate(one_hot_enconding[letter_index], lstm_output_without_padding[i]) for letter_index in x_train_letter_indices[i]] for i in range(len(dataset.x_train_letters))]\n",
    "\n",
    "x_train_letter_concatenated = []\n",
    "\n",
    "for i in range(len(dataset.x_train_letters)):\n",
    "    concatenated_arrays = []\n",
    "    for letter in dataset.x_train_letters[i]:\n",
    "        concatenated_arrays.append(np.concatenate([one_hot_enconding[dataset.arabic_alphabet[letter]], lstm_output_without_padding[i]]))\n",
    "    x_train_letter_concatenated.extend(concatenated_arrays)\n",
    "\n",
    "\n",
    "#[['ق', 'و', 'ل', 'ه'], ['أ', 'و'], ['ق', 'ط', 'ع'], ['ا', 'ل', 'أ', 'و', 'ل']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_letter = [dataset.harakat[diacritic] for diacritic in dataset.y_train_letters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sohad\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bi_lstm_model_char = Sequential()\n",
    "first_bidirectional_layer = Bidirectional(LSTM(units=64, return_sequences=True))\n",
    "bi_lstm_model_char.add(Dense(255, activation='softmax'))\n",
    "\n",
    "bi_lstm_model_char.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bi_lstm_model_char.fit(np.array(x_train_letter_concatenated), np.array(y_train_letter), epochs=10, batch_size=32)\n",
    "score = bi_lstm_model_char.evaluate(np.array(x_train_letter_concatenated), np.array(y_train_letter))\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "bi_lstm_model_char.save(\"char-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_input = Input(shape=(max_word_length,))\n",
    "word_embedding = Embedding(input_dim=num_words, output_dim=embedding_dim)(word_input)\n",
    "word_lstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(word_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
