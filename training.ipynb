{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Team Members</h2>\n",
    "<ul>\n",
    "<li>Sohad Hossam Eldin 1190019</li>\n",
    "<li>Bassant Hisham Mohamed 1190018</li>\n",
    "<li>Yasmin Hashem Niazy 4200013</li>\n",
    "<li>Mary Ashraf 1190322</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        dataset_txt = open(path, \"r\", encoding='utf-8').read()\n",
    "        #print(type(dataset_txt))\n",
    "\n",
    "        chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-]+\\s*/\\s*[a-zA-Z0-9_-]+\\s*\\)|[a-zA-Z0-9_-]+\" \n",
    "        dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "        dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*\",dataset_cleaned)\n",
    "\n",
    "        self.x_train_letters,y_train_letters,self.x_train_words,y_train_words= self.Tokenizing(dataset_sentences)\n",
    "\n",
    "        self.y_train_letters_numbered = LabelEncoder().fit_transform(y_train_letters)\n",
    "        self.y_train_words_numbered = LabelEncoder().fit_transform(y_train_words)\n",
    "\n",
    "        #dictionary of the labels(words,letters) before converted to numbers\n",
    "        self.encoding_mapping_y_letters = {encoded_label: original_label for encoded_label, original_label  in zip(self.y_train_letters_numbered, y_train_letters)}\n",
    "        self.encoding_mapping_y_words = {encoded_label: original_label for encoded_label, original_label  in zip( self.y_train_words_numbered, y_train_words)}  \n",
    "\n",
    "        # print(self.y_train_letters_numbered.shape)\n",
    "        # print(self.y_train_words_numbered.shape)\n",
    "        # print( self.y_train_letters_numbered[:10])\n",
    "        # print( self.y_train_words_numbered[:10])\n",
    "\n",
    "        self.y_train_letters_numbered = self.y_train_letters_numbered.reshape((len(self.y_train_letters_numbered), 1))\n",
    "        self.y_train_words_numbered = self.y_train_words_numbered.reshape((len(self.y_train_words_numbered), 1))\n",
    "\n",
    "        # print(self.y_train_letters_numbered.shape)\n",
    "        # print(self.y_train_words_numbered.shape)\n",
    "        # print( self.y_train_letters_numbered[:10])\n",
    "        # print( self.y_train_words_numbered[:10])\n",
    "\n",
    "\n",
    "    def Tokenizing(self,dataset_sentences):\n",
    "\n",
    "        x_train_letters = []\n",
    "        y_train_letters = []\n",
    "        x_train_words = []\n",
    "        y_train_words = []\n",
    "        for sentence in dataset_sentences:\n",
    "            sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "            tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "            tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "\n",
    "            for word in tokens:\n",
    "                text, inputs, diacritics =util.extract_haraqat(word)\n",
    "                # inputs.insert(0, \"<s>\")\n",
    "                # inputs.append(\"</s>\")\n",
    "\n",
    "                diacritics.insert(0, \"<s>\")\n",
    "                diacritics.append(\"</s>\")\n",
    "                \n",
    "                x_train_letters.append(inputs)\n",
    "                y_train_letters.extend(diacritics)\n",
    "\n",
    "\n",
    "            if(len(tokens)) :\n",
    "                tokens.insert(0, \"<s>\")\n",
    "                tokens.append(\"</s>\")\n",
    "                y_train_words.extend(tokens)\n",
    "            if(len(tokens_wihtout_diacratics)) :\n",
    "                tokens_wihtout_diacratics.insert(0, \"<s>\")\n",
    "                tokens_wihtout_diacratics.append(\"</s>\")\n",
    "                x_train_words.append(tokens_wihtout_diacratics)\n",
    "\n",
    "        \n",
    "        # print(x_train_letters[0:10])\n",
    "        # print(y_train_letters[0:10])\n",
    "        # print(y_train_words[0:10])\n",
    "        # print(x_train_words[0:10])\n",
    "        return x_train_letters,y_train_letters,x_train_words,y_train_words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train_letters), len(self.x_train_words)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train_letters[idx], self.y_train_letters_numbered[idx], self.x_train_words[idx], self.y_train_words_numbered[idx]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    # def get_splits(self, n_test=0.33):\n",
    "    #     # determine sizes\n",
    "    #     test_size = round(n_test * len(self.X))\n",
    "    #     train_size = len(self.X) - test_size\n",
    "    #     # calculate the split\n",
    "    #     return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reading the dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_txt = open(r\"train.txt\", \"r\", encoding='utf-8').read()\n",
    "# print(type(dataset_txt))\n",
    "#print(dataset_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cleaning the dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need to remove [ :ص]\n",
    "# chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-]+\\s*/\\s*[a-zA-Z0-9_-]+\\s*\\)|[a-zA-Z0-9_-]+\" \n",
    "# dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "#print(dataset_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Segmenting dataset into sentences</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r'(\\([^)]*\\))' should we remove the brackets or not\n",
    "# dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*\",dataset_cleaned)\n",
    "# print(type(dataset_sentences))\n",
    "# print(dataset_sentences[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tokenizing the sentence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_letters = []\n",
    "# y_train_letters = []\n",
    "# x_train_words = []\n",
    "# y_train_words = []\n",
    "# for sentence in dataset_sentences:\n",
    "#     sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "#     tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "#     tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "   \n",
    "#     for word in tokens:\n",
    "#         text, inputs, diacritics =util.extract_haraqat(word)\n",
    "#         x_train_letters.append(inputs)\n",
    "#         y_train_letters.append(diacritics)\n",
    "\n",
    "#     if(len(tokens)) :\n",
    "#         tokens.insert(0, \"<s>\")\n",
    "#         tokens.append(\"</s>\")\n",
    "#         y_train_words.append(tokens)\n",
    "#     if(len(tokens_wihtout_diacratics)) :\n",
    "#         tokens_wihtout_diacratics.insert(0, \"<s>\")\n",
    "#         tokens_wihtout_diacratics.append(\"</s>\")\n",
    "#         x_train_words.append(tokens_wihtout_diacratics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_letters[0:10])\n",
    "# print(y_train_letters[0:10])\n",
    "# print(y_train_words[0:10])\n",
    "# print(x_train_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train_letters))\n",
    "# print(x_train_letters[2])\n",
    "\n",
    "#ouh lala , tokinze is dividing the tashkelat as indpendent letters as well , what does that tell us tho "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = ArabicDataset(r\"train.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2]\n",
      " [ 6]\n",
      " [16]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [16]\n",
      " [ 1]]\n",
      "[[     6]\n",
      " [ 91505]\n",
      " [     5]\n",
      " [     6]\n",
      " [  6306]\n",
      " [ 91125]\n",
      " [ 21191]\n",
      " [154522]\n",
      " [  9139]\n",
      " [     5]]\n",
      "[['<s>', 'قوله', '</s>'], ['<s>', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', '</s>'], ['<s>', 'قال', 'الزركشي', '</s>'], ['<s>', 'ابن', 'عرفة', '</s>'], ['<s>', 'قوله', '</s>'], ['<s>', 'بلفظ', 'يقتضيه', 'كإنكار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوبه', 'من', 'الدين', 'ضرورة', '</s>'], ['<s>', 'كإلقاء', 'مصحف', 'بقذر', 'وشد', 'زنار', '</s>'], ['<s>', 'ابن', 'عرفة', '</s>'], ['<s>', 'قول', 'ابن', 'شاس', '</s>'], ['<s>', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلبس', 'الزنار', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجاسة', 'والسجود', 'للصنم', 'ونحو', 'ذلك', '</s>']]\n",
      "[['ق', 'و', 'ل', 'ه'], ['أ', 'و'], ['ق', 'ط', 'ع'], ['ا', 'ل', 'أ', 'و', 'ل'], ['ي', 'د', 'ه'], ['إ', 'ل', 'خ'], ['ق', 'ا', 'ل'], ['ا', 'ل', 'ز', 'ر', 'ك', 'ش', 'ي'], ['ا', 'ب', 'ن'], ['ع', 'ر', 'ف', 'ة']]\n",
      "قوله: [-1.3850944e+00 -1.0189167e+00  1.1170855e+00  2.5235531e-01\n",
      " -1.9094473e+00 -2.9854175e-01  1.6019277e-02  1.1227366e+00\n",
      "  6.2939316e-01 -1.9648057e+00 -2.7973512e-02  5.6716657e-01\n",
      " -6.5277123e-01  2.0180347e+00  2.2122541e-03 -3.8070664e-01\n",
      "  5.1921552e-01 -1.0678059e+00 -6.2613517e-01  1.4867157e-01\n",
      "  1.0477939e+00  5.2944738e-01  2.5588069e+00 -8.5593128e-01\n",
      "  1.0190139e+00  3.2293209e-01 -1.1413349e+00  1.1183439e+00\n",
      " -8.8171542e-01  3.3337754e-01  6.6240960e-01  8.5102838e-01\n",
      " -3.2246247e-01 -9.0164524e-01  1.3422805e-01  3.9668003e-01\n",
      "  4.8242709e-01 -1.6077772e+00 -1.0062437e+00 -9.0114033e-01\n",
      "  9.8540150e-02  2.0158894e-01  1.5429173e+00 -1.0865312e-01\n",
      "  8.1537932e-02 -9.7723693e-02 -6.9147277e-01  6.0758585e-01\n",
      "  6.1469495e-01 -5.5035617e-02  9.5577025e-01  9.4482504e-02\n",
      "  6.2871283e-01  3.6474261e-01 -5.0540280e-01  5.2978861e-01\n",
      "  1.0779305e+00 -1.6976687e-01  1.9182526e-01  4.6937773e-01\n",
      " -7.2882277e-01 -2.8329673e-01 -8.1285495e-01  3.8212693e-01\n",
      "  3.1147805e-01  1.2015803e+00 -2.2627681e-01 -4.4662678e-01\n",
      "  1.0109308e+00  7.3190880e-01 -1.1383065e+00  7.7252138e-01\n",
      "  1.3868276e+00  9.4969994e-01 -1.2959482e-01 -1.0162755e+00\n",
      " -1.6371986e-01 -1.1360167e+00 -5.8648503e-01 -1.2200026e-01\n",
      " -1.6674057e+00  3.2634839e-01  1.3850400e+00  4.6266159e-01\n",
      "  2.7597228e-01 -8.8356435e-01  7.4100018e-01  2.5946587e-01\n",
      " -6.7258257e-01  1.5538760e+00  1.0014977e+00  5.1299836e-02\n",
      "  7.9820126e-02 -3.1518945e-01  9.1479194e-01  1.8454628e+00\n",
      "  9.1310531e-01 -1.1648939e+00 -1.0371253e+00 -1.6009913e-01]\n"
     ]
    }
   ],
   "source": [
    "dataset = ArabicDataset(r\"train.txt\")\n",
    "\n",
    "print(dataset.y_train_letters_numbered[:10])\n",
    "print(dataset.y_train_words_numbered[:10])\n",
    "print(dataset.x_train_words[:10])\n",
    "print(dataset.x_train_letters[:10])\n",
    "\n",
    "\n",
    "# word2vec_model = Word2Vec(\n",
    "#     x_train_words,  # list of sentences, if you don't have all the data in RAM, you can give a file name to corpus_file\n",
    "#     vector_size=50,  # output size of word embedding\n",
    "#     window=4,  # window size\n",
    "#     min_count=1,  # ignores all the words with total frequency lower than this\n",
    "#     workers=5,  # number of workers to use\n",
    "#     sg=1,  # skip-gram\n",
    "#     hs=0,  # 1 --> hierarchical, 0 --> negative sampling\n",
    "#     negative=5,  # how many negative samples\n",
    "#     alpha=0.03,  # the initial learning rate\n",
    "#     min_alpha=0.0001,  # learning rate will linearly drop to min_alpha as training progresses\n",
    "#     seed=54,  # random seed\n",
    "#     iter=10,\n",
    "#     compute_loss=True,  # number of iterations\n",
    "# )\n",
    "model = Word2Vec(sentences=dataset.x_train_words, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "\n",
    "model.save(\"my_word2vec_model\")\n",
    "\n",
    "\n",
    "\n",
    "specific_word = 'قوله'\n",
    "print(f\"{specific_word}: {model.wv[specific_word]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# applying Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "  11/6804 [..............................] - ETA: 12:13 - loss: -70346.6797 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense,Embedding\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "word2vec_model = Word2Vec.load(\"my_word2vec_model\")\n",
    "embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size))\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "x_train_indices = [[word_index[word] for word in sentence] for sentence in dataset.x_train_words]\n",
    "\n",
    "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train_indices, padding='post')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size, weights=[embedding_matrix], input_length=x_train_padded.shape[1], trainable=False))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_padded, dataset.y_train_words_numbered, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(\"my_word2vec_model\")\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size))\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
