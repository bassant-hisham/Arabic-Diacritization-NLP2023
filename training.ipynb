{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Team Members</h2>\n",
    "<ul>\n",
    "<li>Sohad Hossam Eldin 1190019</li>\n",
    "<li>Bassant Hisham Mohamed 1190018</li>\n",
    "<li>Yasmin Hashem Niazy 4200013</li>\n",
    "<li>Mary Ashraf 1190322</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Imports</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArabicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "\n",
    "        dataset_txt = open(path, \"r\", encoding='utf-8').read()\n",
    "        #print(type(dataset_txt))\n",
    "\n",
    "        chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-]+\\s*/\\s*[a-zA-Z0-9_-]+\\s*\\)|[a-zA-Z0-9_-]+\" \n",
    "        dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "        dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*\",dataset_cleaned)\n",
    "\n",
    "        self.x_train_letters,y_train_letters,self.x_train_words,y_train_words= self.Tokenizing(dataset_sentences)\n",
    "\n",
    "        self.y_train_letters_numbered = LabelEncoder().fit_transform(y_train_letters)\n",
    "        self.y_train_words_numbered = LabelEncoder().fit_transform(y_train_words)\n",
    "\n",
    "        #dictionary of the labels(words,letters) before converted to numbers\n",
    "        self.encoding_mapping_y_letters = {encoded_label: original_label for encoded_label, original_label  in zip(self.y_train_letters_numbered, y_train_letters)}\n",
    "        self.encoding_mapping_y_words = {encoded_label: original_label for encoded_label, original_label  in zip( self.y_train_words_numbered, y_train_words)}  \n",
    "\n",
    "        # print(self.y_train_letters_numbered.shape)\n",
    "        # print(self.y_train_words_numbered.shape)\n",
    "        # print( self.y_train_letters_numbered[:10])\n",
    "        # print( self.y_train_words_numbered[:10])\n",
    "\n",
    "        self.y_train_letters_numbered = self.y_train_letters_numbered.reshape((len(self.y_train_letters_numbered), 1))\n",
    "        self.y_train_words_numbered = self.y_train_words_numbered.reshape((len(self.y_train_words_numbered), 1))\n",
    "\n",
    "        # print(self.y_train_letters_numbered.shape)\n",
    "        # print(self.y_train_words_numbered.shape)\n",
    "        # print( self.y_train_letters_numbered[:10])\n",
    "        # print( self.y_train_words_numbered[:10])\n",
    "\n",
    "\n",
    "    def Tokenizing(self,dataset_sentences):\n",
    "\n",
    "        x_train_letters = []\n",
    "        y_train_letters = []\n",
    "        x_train_words = []\n",
    "        y_train_words = []\n",
    "        for sentence in dataset_sentences:\n",
    "            sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "            tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "            tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "\n",
    "            for word in tokens:\n",
    "                text, inputs, diacritics =util.extract_haraqat(word)\n",
    "                # inputs.insert(0, \"<s>\")\n",
    "                # inputs.append(\"</s>\")\n",
    "\n",
    "                diacritics.insert(0, \"<s>\")\n",
    "                diacritics.append(\"</s>\")\n",
    "                \n",
    "                x_train_letters.append(inputs)\n",
    "                y_train_letters.extend(diacritics)\n",
    "\n",
    "\n",
    "            if(len(tokens)) :\n",
    "                tokens.insert(0, \"<s>\")\n",
    "                tokens.append(\"</s>\")\n",
    "                y_train_words.extend(tokens)\n",
    "            if(len(tokens_wihtout_diacratics)) :\n",
    "                tokens_wihtout_diacratics.insert(0, \"<s>\")\n",
    "                tokens_wihtout_diacratics.append(\"</s>\")\n",
    "                x_train_words.append(tokens_wihtout_diacratics)\n",
    "\n",
    "        \n",
    "        # print(x_train_letters[0:10])\n",
    "        # print(y_train_letters[0:10])\n",
    "        # print(y_train_words[0:10])\n",
    "        # print(x_train_words[0:10])\n",
    "        return x_train_letters,y_train_letters,x_train_words,y_train_words\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train_letters), len(self.x_train_words)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train_letters[idx], self.y_train_letters_numbered[idx], self.x_train_words[idx], self.y_train_words_numbered[idx]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    # def get_splits(self, n_test=0.33):\n",
    "    #     # determine sizes\n",
    "    #     test_size = round(n_test * len(self.X))\n",
    "    #     train_size = len(self.X) - test_size\n",
    "    #     # calculate the split\n",
    "    #     return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reading the dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_txt = open(r\"train.txt\", \"r\", encoding='utf-8').read()\n",
    "# print(type(dataset_txt))\n",
    "#print(dataset_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cleaning the dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need to remove [ :ص]\n",
    "# chars_to_remove = r\"\\(\\s*[a-zA-Z0-9_-]+\\s*/\\s*[a-zA-Z0-9_-]+\\s*\\)|[a-zA-Z0-9_-]+\" \n",
    "# dataset_cleaned = re.sub(chars_to_remove, \"\", dataset_txt)\n",
    "#print(dataset_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Segmenting dataset into sentences</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r'(\\([^)]*\\))' should we remove the brackets or not\n",
    "# dataset_sentences = re.split(r\"\\s*\\.\\s*|\\n|\\s*،\\s*|\\s*:\\s*|\\s*[()]\\s*|\\s*؛\\s*|\\s*؟\\s*|\\s*!\\s*|\\s*\\\"\\s*\",dataset_cleaned)\n",
    "# print(type(dataset_sentences))\n",
    "# print(dataset_sentences[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Tokenizing the sentence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_letters = []\n",
    "# y_train_letters = []\n",
    "# x_train_words = []\n",
    "# y_train_words = []\n",
    "# for sentence in dataset_sentences:\n",
    "#     sentence_wihout_diacratics = strip_tashkeel(sentence)\n",
    "#     tokens = word_tokenize(sentence, language=\"arabic\", preserve_line=True)\n",
    "#     tokens_wihtout_diacratics = word_tokenize(sentence_wihout_diacratics, language=\"arabic\", preserve_line=True)\n",
    "   \n",
    "#     for word in tokens:\n",
    "#         text, inputs, diacritics =util.extract_haraqat(word)\n",
    "#         x_train_letters.append(inputs)\n",
    "#         y_train_letters.append(diacritics)\n",
    "\n",
    "#     if(len(tokens)) :\n",
    "#         tokens.insert(0, \"<s>\")\n",
    "#         tokens.append(\"</s>\")\n",
    "#         y_train_words.append(tokens)\n",
    "#     if(len(tokens_wihtout_diacratics)) :\n",
    "#         tokens_wihtout_diacratics.insert(0, \"<s>\")\n",
    "#         tokens_wihtout_diacratics.append(\"</s>\")\n",
    "#         x_train_words.append(tokens_wihtout_diacratics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train_letters[0:10])\n",
    "# print(y_train_letters[0:10])\n",
    "# print(y_train_words[0:10])\n",
    "# print(x_train_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train_letters))\n",
    "# print(x_train_letters[2])\n",
    "\n",
    "#ouh lala , tokinze is dividing the tashkelat as indpendent letters as well , what does that tell us tho "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = ArabicDataset(r\"train.txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2]\n",
      " [ 6]\n",
      " [16]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [16]\n",
      " [ 1]]\n",
      "[[     6]\n",
      " [ 91505]\n",
      " [     5]\n",
      " [     6]\n",
      " [  6306]\n",
      " [ 91125]\n",
      " [ 21191]\n",
      " [154522]\n",
      " [  9139]\n",
      " [     5]]\n",
      "[['<s>', 'قوله', '</s>'], ['<s>', 'أو', 'قطع', 'الأول', 'يده', 'إلخ', '</s>'], ['<s>', 'قال', 'الزركشي', '</s>'], ['<s>', 'ابن', 'عرفة', '</s>'], ['<s>', 'قوله', '</s>'], ['<s>', 'بلفظ', 'يقتضيه', 'كإنكار', 'غير', 'حديث', 'بالإسلام', 'وجوب', 'ما', 'علم', 'وجوبه', 'من', 'الدين', 'ضرورة', '</s>'], ['<s>', 'كإلقاء', 'مصحف', 'بقذر', 'وشد', 'زنار', '</s>'], ['<s>', 'ابن', 'عرفة', '</s>'], ['<s>', 'قول', 'ابن', 'شاس', '</s>'], ['<s>', 'أو', 'بفعل', 'يتضمنه', 'هو', 'كلبس', 'الزنار', 'وإلقاء', 'المصحف', 'في', 'صريح', 'النجاسة', 'والسجود', 'للصنم', 'ونحو', 'ذلك', '</s>']]\n",
      "[['ق', 'و', 'ل', 'ه'], ['أ', 'و'], ['ق', 'ط', 'ع'], ['ا', 'ل', 'أ', 'و', 'ل'], ['ي', 'د', 'ه'], ['إ', 'ل', 'خ'], ['ق', 'ا', 'ل'], ['ا', 'ل', 'ز', 'ر', 'ك', 'ش', 'ي'], ['ا', 'ب', 'ن'], ['ع', 'ر', 'ف', 'ة']]\n"
     ]
    }
   ],
   "source": [
    "dataset = ArabicDataset(r\"train.txt\")\n",
    "\n",
    "print(dataset.y_train_letters_numbered[:10])\n",
    "print(dataset.y_train_words_numbered[:10])\n",
    "print(dataset.x_train_words[:10])\n",
    "print(dataset.x_train_letters[:10])\n",
    "\n",
    "\n",
    "# word2vec_model = Word2Vec(\n",
    "#     x_train_words,  # list of sentences, if you don't have all the data in RAM, you can give a file name to corpus_file\n",
    "#     vector_size=50,  # output size of word embedding\n",
    "#     window=4,  # window size\n",
    "#     min_count=1,  # ignores all the words with total frequency lower than this\n",
    "#     workers=5,  # number of workers to use\n",
    "#     sg=1,  # skip-gram\n",
    "#     hs=0,  # 1 --> hierarchical, 0 --> negative sampling\n",
    "#     negative=5,  # how many negative samples\n",
    "#     alpha=0.03,  # the initial learning rate\n",
    "#     min_alpha=0.0001,  # learning rate will linearly drop to min_alpha as training progresses\n",
    "#     seed=54,  # random seed\n",
    "#     iter=10,\n",
    "#     compute_loss=True,  # number of iterations\n",
    "# )\n",
    "model1 = Word2Vec(dataset.x_train_words, min_count = 1, vector_size = 100, window = 5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
